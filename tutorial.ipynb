{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db14bfe",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "In this tutorial we will work with one Evasion Attack (FSGM) and two Backdoor Attacks. The first part of the notebook will guide you through the implementation of the *Fast Gradient Sign Method (FGSM)*. We will implement it from scratch and also use an existing library to get the same resutls. In the second part of the notebook we will work on two different backdoor attacks. In all experiments we will use the MNIST dataset and a simple convolutional neural network to avoid long training times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95ebbe4",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Lets first import important packages. This includes the package [torchattacks](https://adversarial-attacks-pytorch.readthedocs.io/en/latest/attacks.html), a PyTorch library that provides adversarial attacks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c986e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting and computing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# PyTorch packages\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy, softmax\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# This will install torchattacks if not yet present\n",
    "#!pip install torchattacks\n",
    "import torchattacks\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# For Load Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for image loading\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2aaf85-109a-4493-aeaa-c2de39ddbb09",
   "metadata": {},
   "source": [
    "### Device\n",
    "\n",
    "We also set the device variable so that we can easily switch from using cpu to gpu (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what device we are using\n",
    "use_cuda=True\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ce7b3-9b17-4c63-a224-1a3aaf8eb90a",
   "metadata": {},
   "source": [
    "### Random Seed\n",
    "\n",
    "Execute the code snippet below to set the random seed. This ensures that we can reproduce results over multiple tries. So anyone who re-runs your code will get the exact same outputs.\n",
    "\n",
    "For example: we will set shuffle to True and so the training loader will randomly shuffle the data over multiple runs. If you make changes to your code because training is not going well, then setting the random seed ensures that you can perform the training with the same samples as in previous tries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dca727f-c9ec-4a35-8d7e-42f0ffc88ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this method to be able to reproduce results over multiple tries\n",
    "def setup_seed(seed):\n",
    "    # Set the seed for random number generation on all devices.\n",
    "    torch.manual_seed(seed)\n",
    "    # Numpy module.\n",
    "    np.random.seed(seed)\n",
    "    # Python random module.\n",
    "    random.seed(seed)\n",
    "    # GPU operations have a separate seed we also want to set\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "        # We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "setup_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560187b",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "MNIST is a small dataset with handwritten digits (0-9, 10 classes) that consists of 60K training images and 10K testing images with dimensions of 28x28. It is used for illustrative purposes because it does not need long training times. We use PyTorch's `DataLoader` class to create objects that we use to sample training and test data using batches of size 128.\n",
    "\n",
    "However, we set the batch size for the test loader to 1. This is uncommon, but it is needed as we will loop over all test samples individual to compute the accuracy of the model when the attack has been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba413523",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "img_size  = 28\n",
    "channel   = 1\n",
    "num_workers = 0\n",
    "train_size = 0.8\n",
    "val_size = round(1 - train_size, 2)\n",
    "\n",
    "# A method to ensure reproducibility\n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root='./data', transform=transforms.ToTensor(), download=True, train=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_set, [int(len(train_set)*train_size), int(len(train_set)*val_size)])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=True, worker_init_fn=_init_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=num_workers, pin_memory=True, worker_init_fn=_init_fn)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root='./data', transform=transforms.ToTensor(), download=True, train=False)\n",
    "test_loader = DataLoader(test_set,  batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=True, worker_init_fn=_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e7717",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "Here we provide a basic CNN network that we will train on the MNIST dataset and then attack using evasion (FGSM) and backdoor attacks (BadNets, Blended). Feel free to alter the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a363f99d-ecf7-45fd-97a2-e49d41097c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # padding added to match input size of MNIST\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2) \n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Adjusting the size of the first fully connected layer to match the output from the conv layers\n",
    "        # 5x5 is the spatial size of the image after conv and pooling layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "\n",
    "        # TODO: How many outputs do we need from our model and why? Fill in the appropriate value \n",
    "        self.fc3 = nn.Linear(84, ...) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # No ReLU after this line, output logits directly\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886f574-e680-4e30-8fc1-26709b33f242",
   "metadata": {},
   "source": [
    "We also define an optimizer and loss function with hyper parameter settings for training. Again feel free to make changes to these settings, but for the purpose of this tutorial you can use these pre-defined settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4aa2d4-c5af-4fc8-b8f8-6ea36be8c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_model = Model().to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(clean_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b0bbea",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a8a17",
   "metadata": {},
   "source": [
    "To train our own model we use the following methods:\n",
    "\n",
    "- `accuracy(predictions, labels)`: Takes the two tensors `predictions` and `labels` as input and computes the total number of correct predictions divided by the total number of predictions. This Float value is returned.\n",
    "- `log_training(batch_idx, running_loss, running_acc)`: Takes Integer `batch_idx` which specifies the batch index, Float `running_loss` which represents the loss at that moment of the training, Float `running_acc` which represents the accuracy at that moment of the training. It simply prints these values.\n",
    "- `training_step(model, batch, criterion)`: Takes PyTorch `model`, Tuple `batch` which contains image(s) and label(s), Torch Loss Function `criterion`. This method uses these values to generate predictions, calculate the loss and also the accuracy. It then returns both Float loss and  Float accuracy.\n",
    "- `validation_step(model, batch, criterion)`: Takes PyTorch `model`, Tuple `batch` which contains image(s) and label(s), Torch Loss Function `criterion`. This method uses these values to generate predictions, calculate the loss and also the accuracy. It then returns both Float loss and  Float accuracy inside a Dict.\n",
    "- `validate(model, val_loader, criterion)`: Takes PyTorch `model`, PyTorch DataLoader `val_loader`, Torch Loss Function `criterion`. This method goes over all batches in the val_loader and performs the validation_step. After this it computes the total epoch loss and accuracy by taking the mean over all batch values. These both Float values are returned inside a Dict.\n",
    "- `epoch_end(result)`: Takes Dict `result` which holds the epoch loss and epoch accuracy values. Both values are simply printed.\n",
    "- `train(model, model_name, criterion, optimizer, train_loader, val_loader, num_epochs=10)`: Takes PyTorch `model`, String `model_name`, PyTorch Loss function `criterion`, PyTorch optimizer function `optimizer`, PyTorch DataLoader `train_loader`, PyTorch DataLoader `val_loader`, Integer `num_epochs`. Performs training of model. Model name is used to save the trained model. Loops over all batches in train loader and val loader. Num epochs determines for how many epochs, default is set to 10. Returns `history` which contains all epochs losses and accuracies.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f8bb3-f721-4eb5-b405-d2a65393842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to compute accuracy\n",
    "def accuracy(predictions, labels):\n",
    "    _, preds = torch.max(predictions, dim=1)\n",
    "    return (torch.tensor(torch.sum(preds == labels).item() / len(preds)))\n",
    "\n",
    "# Method to log training / running loss and accuracy\n",
    "def log_training(batch_idx, running_loss, running_acc):\n",
    "    print(f\"Batch: {batch_idx}, Running Loss: {running_loss / (batch_idx + 1):.2f}, Running Accuracy: {running_acc:.2f}\")\n",
    "\n",
    "def training_step(model, batch, criterion):\n",
    "    # Prepare batch data\n",
    "    images, labels = batch\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # TODO: Generate predictions for the batch images\n",
    "    predictions = ...\n",
    "    \n",
    "    # TODO: Calculate loss for the predictions\n",
    "    loss = ...\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy(predictions, labels)\n",
    "    return loss, acc\n",
    "\n",
    "def validation_step(model, batch, criterion):\n",
    "    # Prepare batch data\n",
    "    images, labels = batch\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # TODO: Generate predictions for the batch images\n",
    "    predictions = ...\n",
    "    # TODO: Calculate loss\n",
    "    loss = ...\n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    acc = accuracy(predictions, labels)\n",
    "    return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = [validation_step(model, batch, criterion) for batch in val_loader]\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "# Method to log epoch loss and accuracy\n",
    "def epoch_end(result):\n",
    "    print(f\"val_loss: {result['val_loss']:.2f}, val_acc: {result['val_acc']:.2f}\\n\")\n",
    "\n",
    "def train(model, model_name, criterion, optimizer, train_loader, val_loader, num_epochs=10, scheduler=None):\n",
    "    history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        running_loss = 0\n",
    "        # Training Phase\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            \n",
    "            # TODO: Calculate training loss for this step.\n",
    "            loss, running_acc = ...\n",
    "            \n",
    "            # Compute Gradients\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Reset Gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 50 == 0 and batch_idx != 0:\n",
    "                log_training(batch_idx, running_loss, running_acc)\n",
    "\n",
    "        # Scheduling learning rate by stepLR\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Validation Phase\n",
    "        result = validate(model, val_loader, criterion)\n",
    "        epoch_end(result)\n",
    "        history.append(result)\n",
    "\n",
    "    # Save checkpoint file\n",
    "    torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91af6b",
   "metadata": {},
   "source": [
    "Now train the model for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d5d42-eabf-4d63-be15-90cb553f40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(clean_model, \"clean_model\", criterion, optimizer, train_loader, val_loader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff03a40-59b3-42d7-96aa-6fc2ee235d0f",
   "metadata": {},
   "source": [
    "We name this model \"clean_model\" as we will also train later a \"bckd_model\" to show how simple data poisoning inserts the backdoor functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92f93f",
   "metadata": {},
   "source": [
    "## Evasion Attack - FGSM\n",
    "\n",
    "Now that we constructed and trained a basic CNN we will first start with an evasion attack: FGSM. Below you see the famous panda example. Here we have $x$, which is the original image, and we add $\\epsilon \\cdot sign(\\bigtriangledown_x \\mathcal{J}(\\theta,x, y))$ to get the adversarial image $x'$. Where $y$ is the ground truth label for $x$, $\\theta$ represents the model parameters, and $\\mathcal{J}(\\theta,x, y)$ is the loss that is used to train the network. To calculate $\\bigtriangledown_x \\mathcal{J}(\\theta,x, y)$, the attack backpropagates the gradient back to the input data. Then it uses $\\epsilon$ to adjust the input data by a small step in the direction (i.e. $sign(\\bigtriangledown_x \\mathcal{J}(\\theta,x, y))$) that maximizes the loss. All together:\n",
    "\n",
    "$$x' = x + \\epsilon \\cdot sign(\\bigtriangledown_x \\mathcal{J}(\\theta,x, y))$$\n",
    "\n",
    "![FGSM](images/fgsm_panda_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68732da",
   "metadata": {},
   "source": [
    "### Epsilon\n",
    "\n",
    "Just like in the official [PyTorch FGSM tutorial](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html), we define a list of epsilon values to use during the attack. It includes 0 to represent the model performance on the original test set and we build up to higher values to see what effect this has on the accuracy and on the images. The idea is that the larger the epsilon, the more noticeable the perturbations but the more effective the attack in terms of degrading model accuracy. Since the data range here is $[0,1]$, no epsilon value should exceed 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3111853",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d72fa",
   "metadata": {},
   "source": [
    "### Visualization perturbation FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0fc280",
   "metadata": {},
   "source": [
    "Let's take a batch of images from MNIST test set and plot them before we use FGSM on them and after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images,labels):\n",
    "    np_images = images.detach().cpu().numpy()\n",
    "\n",
    "    # making sure we can view the images\n",
    "    np_images = np_images*255\n",
    "    np_images = [image.astype(np.uint8).reshape((28, 28, 1)) for image in np_images]\n",
    "\n",
    "    # plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    for idx in np.arange(20):\n",
    "        ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "        ax.imshow(np_images[idx], cmap='gray')\n",
    "        # print out the correct label for each image\n",
    "        # .item() gets the value contained in a Tensor\n",
    "        ax.set_title(str(labels[idx].item()))\n",
    "        \n",
    "def plot_image(image,label):\n",
    "    figure = plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image[0].cpu(), cmap='gray')\n",
    "    plt.title(str(label.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bef2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images and labels\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "plot_images(images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccef2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(images[0],labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbc967",
   "metadata": {},
   "source": [
    "Now lets compute the adverserial images for this batch using FGSM. \n",
    "\n",
    "Below we provide an `FGSM` class which can be used to create an FGSM attack object. The object can be created providing a (trained) model, an epsilon value between 0.0 and 1.0 (default set to 0.3), a loss function (default set to Cross Entropy) and the device (default set to cpu). \n",
    "\n",
    "The `forward()` method of the `FGSM` class can be used to create adversarial images. It takes in images and corresponding labels. It then uses the trained model to generate predictions and compute the loss. Using [`torch.autograd.grad()`](https://pytorch.org/docs/stable/generated/torch.autograd.grad.html) it computes the sum of gradients of the predictions with respect to the inputs. Using this sum of gradients and the epsilon value we can then create adversarial image of the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63894725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGSM(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, eps=0.3, criterion=nn.CrossEntropyLoss(), device='cpu'):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.eps = eps\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, images, labels):\n",
    "        # Prepare data\n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "        # Specify that the gradients need to be computed\n",
    "        images.requires_grad = True\n",
    "        # Generate predictions\n",
    "        predictions = self.model(images)\n",
    "        # Compute Loss\n",
    "        loss = self.criterion(predictions,labels)\n",
    "        # Calculate the derivative of the loss w.r.t. the original image\n",
    "        grad = torch.autograd.grad(loss, images, retain_graph=False, create_graph=False)[0]\n",
    "        # Compute the adversarial images\n",
    "        adv_images = images + self.eps * grad.sign()\n",
    "        adv_images = torch.clamp(adv_images, min=0, max=1).detach()\n",
    "        return adv_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b7494-fa30-459d-9e42-62b439aec83c",
   "metadata": {},
   "source": [
    "Execute the code cell below to create a FGSM attack object with epsilon of 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ce2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = FGSM(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_images = attack(images, labels)\n",
    "plot_images(adv_images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(adv_images[2],labels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d0fb6",
   "metadata": {},
   "source": [
    "As you can see from the last two plots is that using $\\epsilon = 0.3$ causes very noticeable perturbations. The changes to the original images can be spotted quite easily. Lets now check what is the effect on the performance of the used model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb1f14",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0a338",
   "metadata": {},
   "source": [
    "For the purpose of checking the effectiveness of the FGSM attack on the performance of our model we provide you with the `test` method below. This method takes in the `model`, `test_loader` and `epsilon` value. This way we can try out different epsilon values to see how this affects the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_perturbed_image(image, epsilon):\n",
    "    # Collect gradient\n",
    "    image_grad = image.grad.data\n",
    "        \n",
    "    # Collect the element-wise sign of the gradient\n",
    "    sign_image_grad = image_grad.sign()\n",
    "        \n",
    "    # TODO: Create the perturbed image by adjusting each pixel of the input\n",
    "    # image using the FGSM formula\n",
    "    perturbed_image = ....\n",
    "        \n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "\n",
    "    return perturbed_image\n",
    "\n",
    "def save_example(adv_examples, perturbed_image, init_pred, final_pred):\n",
    "    adv_ex = perturbed_image.squeeze().detach().cpu().numpy()\n",
    "    adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "\n",
    "def attack_test(model, criterion, device, test_loader, epsilon):\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    \n",
    "    # Loop over all samples in test set\n",
    "    for image, label in test_loader:\n",
    "        \n",
    "        # Send the data and label to the device\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        \n",
    "        # Set requires_grad attribute to True. Important for Attack\n",
    "        image.requires_grad = True\n",
    "        \n",
    "        # TODO: Forward pass the data through the model\n",
    "        prediction = ...\n",
    "        \n",
    "        # Get the index of the max log-probability\n",
    "        _, init_pred = prediction.max(1, keepdim=True)\n",
    "        \n",
    "        # If the initial prediction is wrong, don't bother attacking, just move on\n",
    "        if init_pred.item() != label.item():\n",
    "            continue\n",
    "            \n",
    "        # TODO: Calculate the loss for this data\n",
    "        loss = ...\n",
    "        \n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute pertrubed image\n",
    "        perturbed_image = create_perturbed_image(image, epsilon)\n",
    "        \n",
    "        # TODO: Re-classify the perturbed image\n",
    "        new_prediction = ...\n",
    "        \n",
    "         # Get the index of the max log-probability\n",
    "        _, final_pred = new_prediction.max(1, keepdim=True)\n",
    "        \n",
    "        # TODO: Check for success. What is the condition to consider the prediction of\n",
    "        # the perturbed image correct?\n",
    "        if ...item() == ...item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                save_example(adv_examples, perturbed_image, init_pred, final_pred)\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                save_example(adv_examples, perturbed_image, init_pred, final_pred)\n",
    "    \n",
    "    # TODO: Calculate final accuracy for this epsilon\n",
    "    final_acc = .../float(...)\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = attack_test(clean_model, criterion, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6e89f",
   "metadata": {},
   "source": [
    "### Accuracy vs Epsilon\n",
    "\n",
    "Like we mentioned earlier as the epsilon increases we expect the test accuracy to decrease. The reason is that with a larger epsilon we take a larger step in the direction that will maximize the loss. Notice the trend in the curve is not linear even though the epsilon values are linearly spaced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac497897",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "# TODO: Plot the epsilons in the x-axis and the accuracies in the y-axis \n",
    "plt.plot(..., ..., \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b7113",
   "metadata": {},
   "source": [
    "While a higher epsilon might decrease the accuracy, the plots below also show that the perturbations become more easily perceptible and thus can be detected easier. So an attacker should consider a tradeoff between accuracy degredation and perceptibility of the perturbations. The plots below show examples of successful adversarial examples at each epsilon value. Each row of the plot shows a different epsilon value. The first row is the $\\epsilon = 0$ examples which represent the original \"clean\" images with no perturbation. The title of each image shows the \"original classification -> adversarial classification\". Notice, the perturbations start to become evidant from around $\\epsilon = 0.1$ and are quite evident at $\\epsilon = 0.3$. However, in all cases humans are still capable of identifying the correct class despite the added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot several examples of adversarial samples at each epsilon\n",
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(examples[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons), len(examples[0]), cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
    "        orig,adv,ex = examples[i][j]\n",
    "        plt.title(\"{} -> {}\".format(orig, adv))\n",
    "        plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed7a0e-9348-48ab-8fab-673d90f8134d",
   "metadata": {},
   "source": [
    "### Attack Evaluation\n",
    "\n",
    "To evaluate the success of your evasion attack you could measure the model's accuracy. First you test your model using clean samples. Then you test your model using your evasion attack and a specific epsilon value. The drop in accuracy rate is the measure of success of your attack. This is calculated in the code snippet below. We can reuse the `attack_test()` method from earlier using `epsilon=0` to test the clean model on only clean samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f8fd4-84cf-435a-8780-d6deb3f55137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use attack_test with the correct arguments to calculate clean test accuracy\n",
    "clean_acc, _ = \n",
    "\n",
    "# TODO: Use attack_test with the correct arguments to calculate attack accuracy\n",
    "attack_acc, _ = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88617d-30e7-4374-918e-dedbf3356462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the accuracy drop\n",
    "acc_drop = round(...,2)\n",
    "print(f\"Accuracy Drop: {acc_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14dde9b-046a-4876-b56d-d3264e56ea3c",
   "metadata": {},
   "source": [
    "If you would do this for all epsilon you could then plot the accuracy drops in a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf78d8-3077-4980-90ac-4946d4c8af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_drops = [round(x - accuracies[0],4) for x in accuracies[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d5098-4968-4ab7-bfea-f83d0e488ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(len(acc_drops)), acc_drops, color='skyblue')\n",
    "\n",
    "# Adding the text inside the bars for each value\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval - 0.03, round(yval, 4), ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Epsilon Value')\n",
    "plt.ylabel('Accuracy Drop')\n",
    "plt.title('Bar Plot Evasion Attack Accuracy Drop')\n",
    "plt.xticks(range(len(acc_drops)), [str(eps) for eps in epsilons[1:]])  # Set x-ticks to be the indices\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2cb61",
   "metadata": {},
   "source": [
    "### Torchattacks\n",
    "\n",
    "Instead of implementing the FGSM attack yourself, you can also make use of PyTorch `torchattack` library that includes many predefined attacks. We will show that our implementation of FGSM attack and the `torchattack` version yield the same results. First we define a `test` method to show the accuracy of the model using the original images and then we define a `adv_test` method to show the accuracy using FGSM adversarial images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a6c49-b303-4aaa-9296-6adb4b0dbd43",
   "metadata": {},
   "source": [
    "Both `test` and `adv_test` do not require to iterate over the images one by one. So, to speed things up we need to use a larger batch size in our test_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e83edb-0c72-49fb-b09d-cee22ca6738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a new testloader with a batch of 128 instead of 1\n",
    "test_loader = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b20f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    print('\\n\\n[Plain/Test] Under Testing ... Please Wait')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(test_loader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Evaluation\n",
    "            predictions = model(images).detach()\n",
    "\n",
    "            # Test\n",
    "            _, predicted = torch.max(predictions, dim=1)\n",
    "            total += labels.numel()\n",
    "            correct += (predicted == labels).sum().item() \n",
    "\n",
    "\n",
    "        print('[Plain/Test] Acc: {:.3f}'.format(100.*correct / total))\n",
    "    return 100.*correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use test to calculate the clean accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868436df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_test(attack, model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    print('\\n\\n[Plain/Test] Under Testing ... Please Wait')\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(test_loader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # TODO: generate adversarial examples for the batch images\n",
    "        adv_inputs = ... \n",
    "\n",
    "        # Evaluation\n",
    "        predictions = model(adv_inputs).detach()\n",
    "\n",
    "        # Test\n",
    "        _, predicted = torch.max(predictions, dim=1)\n",
    "        total += labels.numel()\n",
    "        correct += (predicted == labels).sum().item() \n",
    "\n",
    "        \n",
    "    print('[Plain/Test] Acc: {:.3f}'.format(100.*correct / total))\n",
    "    return 100.*correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bfd04-bf44-4e9c-809c-1319dab0f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a loop that iterates over all epsilons, defines an object of our attack,\n",
    "# and prints the attack accuray (using adv_test) for this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb462c0c-c0a6-49c1-b271-bff54838fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a loop that iterates over all epsilons, defines an object of \n",
    "# torchattacks.FGSM(<model>, <epsilon>), and prints the attack accuray (using adv_test) \n",
    "# for this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef32c1a6",
   "metadata": {},
   "source": [
    "As you can see both lower the performance of our model to the same accuracy and with the `torchattacks` library you only need to provide a pre-trained model and an epsilon value to define the FGSM attack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9bf745",
   "metadata": {},
   "source": [
    "# Backdoor Attacks\n",
    "\n",
    "In this section we will discuss how to perform several kinds of backdoor attacks. In the case of a backdoor attack, the adversary has access to (only a small subset of) the training data. By altering a few data samples, the adversary inserts a secret functionality into the trained model. Such secret functionality in a classifier is a targeted misclassification. The secret functionality, or backdoor, is activated when the model's input contains an attacker-chosen property (trigger). \n",
    "\n",
    "You will learn how to apply the [BadNets](https://arxiv.org/pdf/1708.06733.pdf) backdoor attack and the [Blended](https://arxiv.org/pdf/1712.05526.pdf) backdoor attack. With the BadNets  attack, we will insert a small square patch to the input images, while with the Blended attack we will blend the input image with another image. Furthermore, we will learn to apply two different modes of backdoor attacks, i.e. the source-agnostic and source-specific attacks. \n",
    "\n",
    "We will only apply a dirty label static backdoor attack. Dirty label means that you will add the trigger to multiple classes and change their label to the target class. We will perfrom static backdoor attacks, as the trigger will be a static property of the input images and will not change over time.\n",
    "\n",
    "Aside from applying the attacks, we will also learn about metrics used in this field to evaluate the attack. These metrics are the attack success rate (ASR) and the clean accuracy drop.\n",
    "\n",
    "Again we will be using the MNIST dataset and the simple CNN introduced in the beginning of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe568c2",
   "metadata": {},
   "source": [
    "### Visualize Data\n",
    "\n",
    "Below we have added a method to show in a very detailed way an image. To show a batch of images `plot_images` defined above is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_detailed(image,label):\n",
    "    image = image.detach().numpy()\n",
    "    image = np.squeeze(image)\n",
    "    fig = plt.figure(figsize = (12,12)) \n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(f\"Label is {str(label.item())}\")\n",
    "\n",
    "    # annotate each pixel in the image with its value\n",
    "    width, height = image.shape\n",
    "    thresh = image.max()/2.5\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            val = round(image[x][y],2) if image[x][y] !=0 else 0\n",
    "            ax.annotate(str(val), xy=(y,x),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center',\n",
    "                        color='white' if image[x][y]<thresh else 'black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a060b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "plot_images(images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66096e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_image_detailed(images[0],labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10b840",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Below you find a simple test method which you can use to test the performance (accuracy and loss) of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77567373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion, DataLoader):\n",
    "    print('\\n\\n[Plain/Test] Under Testing ... Please Wait')\n",
    "    examples = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(DataLoader)):\n",
    "            # Prepare batch data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Generate predictions\n",
    "            outputs = model(inputs).detach()\n",
    "            \n",
    "            # TODO: Calculate loss for the generated outputs\n",
    "            batch_test_loss = ...\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            batch_test_acc = torch.tensor(torch.sum(predicted == labels).item() / len(predicted))\n",
    "            \n",
    "            # Store batch results\n",
    "            test_loss.append(batch_test_loss)\n",
    "            test_acc.append(batch_test_acc)\n",
    "            # Store Examples\n",
    "            ex = inputs[0].squeeze().detach().cpu().numpy()\n",
    "            examples.append((labels[0],predicted[0],ex))\n",
    "\n",
    "    # Display Results\n",
    "    print(\"Test Loss: \", round(torch.stack(test_loss).mean().item(),2))\n",
    "    print(\"Test Accuracy: \", round(torch.stack(test_acc).mean().item()*100.0,2))\n",
    "    return round(torch.stack(test_acc).mean().item(),4), examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e248f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, examples = test(clean_model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a037ac",
   "metadata": {},
   "source": [
    "## BadNets (Square Trigger)\n",
    "\n",
    "After training and testing your clean model, you will now start with the first backdoor attack by applying square triggers to the training set and training a new backdoored model.\n",
    "\n",
    "### Create Square Trigger\n",
    "You will need to finish the code below. In the code snippet below you will find the class ```GenerateSQRTrigger```, which is an object that can be used to create a square trigger inside an image. This trigger type is one of the first in the literature ([BadNets paper](https://arxiv.org/abs/1708.06733)). \n",
    "\n",
    "It now takes 2 parameters which is ```size``` and ```pos_label```.\n",
    "\n",
    "- ```size``` is a tuple indicating the size of the square trigger and this should not exceed the dimensions of the images in the dataset and it should always be a tuple of identical numbers.\n",
    "- ```pos_label``` is a string that shows the trigger's position.\n",
    "\n",
    "The class also contains the following methods:\n",
    "\n",
    "- ```_gen_pos_square()``` should include code that creates the x and y coordinates inside the image where the square should be created (be aware that you do not exceed the dimensions of the image). \n",
    "\n",
    "- ```create_trigger_square()``` should include code that creates a object with the same dimensions as the images in the dataset and then places the square inside this object. \n",
    "\n",
    "- ```apply_trigger()``` should be used to apply the created square trigger on the actual images of the dataset. This is also used inside the ```poison(img,trigger_obj)``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d34b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison(img, trigger_obj):\n",
    "    \"\"\"Poison the training samples by stamping the trigger.\"\"\"\n",
    "    poisoned_image = trigger_obj.apply_trigger(img)\n",
    "    return poisoned_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateSQRTrigger:\n",
    "    \"\"\"\n",
    "    A class that creates a random square pattern that is used as a trigger for an\n",
    "    image dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, pos_label, dataset='mnist'):\n",
    "\n",
    "        datasets_dimensions = {\"mnist\": (28, 28, 1)}\n",
    "        dims = datasets_dimensions[dataset]\n",
    "\n",
    "        if size[0] != size[1]:\n",
    "            raise Exception(\"The size of the trigger must be square.\")\n",
    "\n",
    "        if pos_label.lower() not in [\"upper-left\", \"upper-mid\", \"upper-right\", \n",
    "                                     \"mid-left\", \"mid-mid\", \"mid-right\",\n",
    "                                     \"lower-left\", \"lower-mid\", \"lower-right\"]:\n",
    "            raise Exception(\n",
    "                (\"The position of the trigger must be one of the following: \"\n",
    "                 \"upper-left, upper-mid, upper-right, mid-left, mid-mid, mid-right, \"\n",
    "                 \"lower-left, lower-mid, lower-right\")\n",
    "            )\n",
    "\n",
    "        if size[0] > dims[0] or size[1] > dims[1]:\n",
    "            raise Exception(\"The size of the trigger is too large for the dataset items.\")\n",
    "\n",
    "        self.dims = dims\n",
    "        self.size = size\n",
    "        self.pos_label = pos_label\n",
    "        # pos == position; coordinates\n",
    "\n",
    "        # TODO: Generate the coordinates of the generated trigger.\n",
    "        self.pos_coords = ...\n",
    "\n",
    "        # Use a black image as a trigger initially.\n",
    "        trigger = np.zeros(self.dims, dtype=np.float32)\n",
    "\n",
    "        # TODO: Use one of the methods defined below to craft the trigger image\n",
    "        self.crafted_trigger = ...\n",
    "\n",
    "    def _gen_pos_square(self):\n",
    "        \"\"\"Returns the coordinates of the generated square trigger.\"\"\"\n",
    "        # Upper\n",
    "        if self.pos_label == \"upper-left\":\n",
    "            return (0, 0)\n",
    "        elif self.pos_label == \"upper-mid\":\n",
    "            return (0, self.dims[1] // 2 - self.size[1] // 2)\n",
    "        elif self.pos_label == \"upper-right\":\n",
    "            return (0, self.dims[1] - self.size[1])\n",
    "\n",
    "        # Mid\n",
    "        elif self.pos_label == \"mid-left\":\n",
    "            return (self.dims[0] // 2 - self.size[0] // 2, 0)\n",
    "        elif self.pos_label == \"mid-mid\":\n",
    "            return (self.dims[0] // 2 - self.size[0] // 2,\n",
    "                    self.dims[1] // 2 - self.size[1] // 2)\n",
    "        elif self.pos_label == \"mid-right\":\n",
    "            return (self.dims[0] // 2 - self.size[0] // 2, self.dims[1] - self.size[1])\n",
    "\n",
    "        # Lower\n",
    "        elif self.pos_label == \"lower-left\":\n",
    "            return (self.dims[0] - self.size[0], 0)\n",
    "        elif self.pos_label == \"lower-mid\":\n",
    "            return (self.dims[0] - self.size[0], self.dims[1] // 2 - self.size[1] // 2)\n",
    "        elif self.pos_label == \"lower-right\":\n",
    "            return (self.dims[0] - self.size[0], self.dims[1] - self.size[1])\n",
    "\n",
    "    def create_trigger_square(self, trigger):\n",
    "        \"\"\"Create a square trigger.\"\"\"\n",
    "        base_x, base_y = self.pos_coords\n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                trigger[base_x + x][base_y + y] = \\\n",
    "                    np.ones((self.dims[2]))\n",
    "\n",
    "        return trigger\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"applies the trigger on the image.\"\"\"\n",
    "\n",
    "        base_x, base_y = self.pos_coords\n",
    "        for x in range(self.size[0]):\n",
    "            for y in range(self.size[1]):\n",
    "                # TODO: Replace the corresponding pixels with the trigger values\n",
    "                img[base_x + x][base_y + y] = ...\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de8a44",
   "metadata": {},
   "source": [
    "### Source-agnostic vs source-specific\n",
    "\n",
    "In **source-agnostic** attacks the trigger is effective in any image, regardless its original class.\n",
    "In **source-specific** attacks the trigger is effective only for a specific original class. If the trigger is applied in any other input the model should classify the poisoned input correctly.\n",
    "\n",
    "Now you should use the code above to poison the training data. You will need to decide on the following settings:\n",
    "- Source specific: Do you want the attack to be source specific, if so then provide a variable with the source label.\n",
    "- Backdoor / target label: If an image includes a trigger, what should the target label be?\n",
    "- Epsilon: specify what percentage of the training set you want to poison.\n",
    "\n",
    "Do the following when poisoning the training dataset in case of a **source agnostic** attack:\n",
    "- If a training set example is part of the subset you want to poison, then poison the image and provide the backdoor label as its new label.\n",
    "- If a training set example is not part of the subset you want to poison, then use the original image and label.\n",
    "\n",
    "Do the following when poisoning the training dataset in case of a **source specific** attack:\n",
    "\n",
    "- If a training set example is part of the subset you want to poison and its original label is equal to the source label, poison the image and provide the backdoor label as its new label.\n",
    "- If a training set example is part of the subset you want to poison and its original label is not equal to the source label, poison the image and provide the original label.\n",
    "- If a training set example is not part of the subset you want to poison, then use the original image and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackdoorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, clean_dataset, trigger_obj, epsilon=None, target_label=None, source_label=None, train=True):\n",
    "        self.clean_dataset = clean_dataset\n",
    "        self.trigger_obj = trigger_obj\n",
    "        self.epsilon = epsilon\n",
    "        self.target_label = target_label\n",
    "        self.source_label = source_label\n",
    "        self.train = train\n",
    "        if train:\n",
    "            self.backdoor_dataset = self.get_train_set()\n",
    "        else:\n",
    "            self.backdoor_dataset = self.get_test_set()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.backdoor_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.backdoor_dataset[idx]\n",
    "        return image, label\n",
    "\n",
    "    def poison(self, img):\n",
    "        \"\"\"Poison a training sample by adding the trigger.\"\"\"\n",
    "        # TODO: Insert the trigger to the given image\n",
    "        return ...\n",
    "    \n",
    "    def get_train_set(self):\n",
    "        backdoored_ds = []\n",
    "\n",
    "        # TODO: Calculate how many samples from the clean_dataset should be poisoned \n",
    "        # based on the poisoning rate (epsilon).\n",
    "        trigger_samples = ...\n",
    "        \n",
    "        # replace=False means that each value cannot be selected more than one time\n",
    "        samples_index = np.random.choice(len(self.clean_dataset), size=trigger_samples, replace=False)\n",
    "        \n",
    "        for idx, (image, label) in enumerate(self.clean_dataset):\n",
    "            poisoned_image = torch.from_numpy(self.poison(image.clone().cpu().permute(1, 2, 0).numpy())).permute(2, 0, 1)\n",
    "            \n",
    "            if self.source_label is not None:\n",
    "                if idx in samples_index:\n",
    "                    if label == self.source_label:\n",
    "                        # TODO: What should the label be in this case?\n",
    "                        insert = (poisoned_image, ...)\n",
    "                    else:\n",
    "                        # TODO: What should the label be in this case? Why?\n",
    "                        insert = (poisoned_image, ...)\n",
    "                else:\n",
    "                    insert = (image, label)\n",
    "            else:\n",
    "                if idx in samples_index:\n",
    "                    insert = (poisoned_image, self.target_label)\n",
    "                else:\n",
    "                    insert = (image, label)\n",
    "            backdoored_ds.append(insert)\n",
    "            \n",
    "        return backdoored_ds\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        backdoored_ds = []\n",
    "        \n",
    "        for idx, (image, label) in enumerate(self.clean_dataset):\n",
    "            poisoned_image = torch.from_numpy(self.poison(image.clone().cpu().permute(1, 2, 0).numpy())).permute(2, 0, 1)\n",
    "\n",
    "            if label != self.target_label:\n",
    "                insert = (poisoned_image, label)\n",
    "                backdoored_ds.append(insert)\n",
    "        return backdoored_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e773bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_obj = GenerateSQRTrigger((4, 4), 'upper-left')\n",
    "\n",
    "target_label = 1\n",
    "source_label = None\n",
    "training_epsilon = 0.06\n",
    "test_epsilon = None\n",
    "batch_size = 128\n",
    "\n",
    "# TODO: Define a backdoored training set\n",
    "bkdr_train_set = BackdoorDataset(....)\n",
    "bkdr_train_loader = torch.utils.data.DataLoader(bkdr_train_set, batch_size=batch_size, shuffle=True,\n",
    "                                                num_workers=num_workers)\n",
    "\n",
    "# TODO: Define a backdoored validation set\n",
    "bkdr_val_set = BackdoorDataset(...)\n",
    "bkdr_val_loader = torch.utils.data.DataLoader(bkdr_val_set, batch_size=batch_size, shuffle=True,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "torch.save(bkdr_train_set, './bkdr_train_set.pt')\n",
    "\n",
    "# TODO: Define a backdoored test set\n",
    "bkdr_test_set = BackdoorDataset(...)\n",
    "bkdr_test_loader = torch.utils.data.DataLoader(bkdr_test_set,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=num_workers)\n",
    "\n",
    "torch.save(bkdr_test_set, './bkdr_test_set.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebbb801-9444-4cd6-b3d0-45bdaae8427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkdr_model = Model().to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(bkdr_model.parameters(),lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# %%\n",
    "train(bkdr_model, \"bkdr_model_badnets\", criterion, optimizer, bkdr_train_loader, bkdr_val_loader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(bkdr_train_loader)\n",
    "images, labels = next(dataiter)\n",
    "plot_images(images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed1ef2-611b-4af9-b3e7-3639c5f6443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(bkdr_test_loader)\n",
    "images, labels = next(dataiter)\n",
    "plot_images(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0c5a6",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "#### Attack Succes Rate\n",
    "\n",
    "Use the method ```calculate_ASR()``` to compute the attack success rate of you backdoored model. \n",
    "\n",
    "You will neeed a trained backdoored model. Then use the method below while providing a **poisoned test set in the form of a DataLoader**. Also provide the same trigger object, backdoor label and original label used during poisoning of training set.\n",
    "\n",
    "The method below assumes a test dataloder from a test dataset of (poisoned_image, original_label) pairs. The reason it assumes original labels instead of target/backdoor labels is to compute the ASR in the source specific case. Here we need to know if the original label was the source label and if that image is then missclassified with the target/backdoor label. This is only assumed for the test set! The train dataset should be of (poisoned_image, target_label) pairs.\n",
    "\n",
    "In case of a source specific attack you compute the ASR only for images with the specific source label. However, you also poison other labeled images. With ```verbose``` set to True the method will print in how many cases of these other labeled images the prediction was equal to the target label instead of the original label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18062b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_source_indices(y_test,source_label):\n",
    "    indices = (y_test == source_label).nonzero(as_tuple=False).numpy()\n",
    "    indices = indices.reshape(indices.shape[0])\n",
    "    return indices\n",
    "\n",
    "def find_non_source_indices(y_test, source_label, target_label):\n",
    "    # get indices of samples which do not have source or target label\n",
    "    indices = torch.logical_and((y_test != source_label),(y_test != target_label)).nonzero(as_tuple=False).numpy()\n",
    "    indices = indices.reshape(indices.shape[0])\n",
    "    return indices\n",
    "\n",
    "def count_non_source_misclassifications(original_labels, predicted, source_label, target_label):\n",
    "    sub_non_source_total = 0\n",
    "    sub_misclassifications = 0\n",
    "\n",
    "    # find all the images with a different label than the source or target label\n",
    "    indices = find_non_source_indices(original_labels,source_label,target_label)\n",
    "    sub_non_source_total += indices.shape[0]\n",
    "\n",
    "    # TODO: For all non-source and non-target label images check if the prediction is equal to the target label\n",
    "    for index in indices:\n",
    "        if ....detach().cpu().numpy() == ...:\n",
    "            sub_misclassifications += 1\n",
    "    return sub_misclassifications, sub_non_source_total\n",
    "\n",
    "def count_source_specific_classifications(original_labels,predicted,source_label,target_label):\n",
    "    sub_total = 0\n",
    "    sub_correct = 0\n",
    "    \n",
    "    # find all the images with the source label\n",
    "    indices = find_source_indices(original_labels, source_label)\n",
    "    sub_total += indices.shape[0]\n",
    "    \n",
    "    # TODO: For all source label images check if the prediction is equal to the target label\n",
    "    for i in indices:\n",
    "        if ....detach().cpu().numpy() == ...:\n",
    "            sub_correct +=1\n",
    "    return sub_correct, sub_total\n",
    "\n",
    "def calculate_ASR(model, dataloader, target_label, source_label=None, verbose=False):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    non_source_total = 0\n",
    "    misclassifications = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for inputs, original_labels in tqdm(dataloader):\n",
    "            # Use poisoned test image to get predictions of backdoored model\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs).detach()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            \n",
    "            # If source specific attack\n",
    "            if source_label is not None:\n",
    "                sub_correct, sub_total = count_source_specific_classifications(original_labels,predicted,source_label,target_label)\n",
    "                correct += sub_correct\n",
    "                total += sub_total\n",
    "                if verbose:\n",
    "                    sub_misclassifications, sub_non_source_total = count_non_source_misclassifications(original_labels,predicted,source_label,target_label)\n",
    "                    misclassifications += sub_misclassifications\n",
    "                    non_source_total += sub_non_source_total\n",
    "            # if source agnostic attack\n",
    "            else:\n",
    "                # for all test samples check if the predicted label is equal to the target label\n",
    "                for i in range(len(inputs)):\n",
    "                    # TODO: if the sample does not come from the target label\n",
    "                    if ... :\n",
    "                        total += 1\n",
    "                        # TODO: if the prediction of the poisoned sample is the target label\n",
    "                        if ....detach().cpu().item() == ... :\n",
    "                            correct += 1\n",
    "\n",
    "    attack_acc = (correct * 100.0) / total\n",
    "    print(f\"Attack accuracy: {round(attack_acc,2)}\")\n",
    "    \n",
    "    if source_label and verbose:\n",
    "        print(misclassifications)\n",
    "        print(non_source_total)\n",
    "        misclassification_rate = (misclassifications * 100.0) / non_source_total\n",
    "        print(f\"False Positive rate: {round(misclassification_rate,2)}\")\n",
    "        \n",
    "    return attack_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ef76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_acc = calculate_ASR(bkdr_model, bkdr_test_loader, target_label=target_label, source_label=source_label, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376518f",
   "metadata": {},
   "source": [
    "#### Clean Accuracy Drop\n",
    "\n",
    "In order to compute the clean accuracy drop you will need to first train a model on the clean training set and use the ```test()``` method provided earlier on in this notebook to compute the clean accuracy. Then train a model using the backdoored/poisoned training set. Again use the same ```test()```method to compute the accuracy of this backdoored model on the clean test set. Compare the accuracies to compute the accuracy drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the accuracy of the clean model on clean data\n",
    "clean_acc, examples = test(...)\n",
    "# TODO: Compute the accuracy of the poisoned model on clean data\n",
    "bkdr_acc, examples = test(...)\n",
    "\n",
    "# TODO: Calculate the clean accuracy drop using 4 decimals\n",
    "print(f\"Clean Accuracy drop of: {round(...,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6e72f",
   "metadata": {},
   "source": [
    "## Blended Backdoor Attack\n",
    "\n",
    "This attack was introduced in [Targeted backdoor attacks through data poisoning](https://arxiv.org/abs/1712.05526) and it was one of the first backdoor generation techniques. As a trigger a base image is \"blended\" with the original inputs. In this paper the authors used 2 images for the blending. One was an image from \"Hello Kitty\" and another was an image with random pixel values. The constructor of the following class needs two arguments, the dataset and the trigger type. The ```trigger_blended``` function loads the correct image and saves it in ```self.crafted_trigger```. The method ```apply_trigger``` blends a given input with the trigger, and returns the result (poisoned image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47988a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateBlendedTrigger:\n",
    "    \"\"\"\n",
    "    A class that uses images of the same dimensions as the dataset as triggers\n",
    "    that will be blended with the clean images.\n",
    "\n",
    "    We will use a random pattern or a hello-kitty image as the original paper\n",
    "    (https://arxiv.org/pdf/1712.05526.pdf).\n",
    "    \"\"\"\n",
    "    hello_kitty_path = \"./images/hello_kitty.jpg\"\n",
    "\n",
    "    def __init__(self, dataset, trigger):\n",
    "\n",
    "        datasets_dimensions = {\"mnist\": (28, 28, 1)}\n",
    "\n",
    "        # Use a hardcoded seed for reproducibility\n",
    "        dims = datasets_dimensions[dataset]\n",
    "\n",
    "        if trigger not in [\"random\", \"hello-kitty\"]:\n",
    "            raise Exception(f\"Pick 'random' or 'hello-kitty' trigger\")\n",
    "\n",
    "        if dataset not in datasets_dimensions:\n",
    "            raise Exception(f\"Dataset is not supported\")\n",
    "\n",
    "        self.dims = dims\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # Generate the correct trigger\n",
    "        self.crafted_trigger = self.trigger_blended(trigger)\n",
    "\n",
    "    def trigger_blended(self, trigger):\n",
    "        \"\"\"Prepare the trigger for blended attack.\"\"\"\n",
    "        if trigger == \"hello-kitty\":\n",
    "            # Load kitty\n",
    "            img = Image.open(self.hello_kitty_path)\n",
    "\n",
    "            # Resize to dimensions\n",
    "            tmp = img.resize(self.dims[:-1])\n",
    "\n",
    "            if self.dims[2] == 1:\n",
    "                tmp = ImageOps.grayscale(tmp)\n",
    "\n",
    "            tmp = np.asarray(tmp)\n",
    "            # This is needed in case the image is grayscale (width x height) to\n",
    "            # add the channel dimension\n",
    "            tmp = tmp.reshape((self.dims))\n",
    "            trigger_array = tmp / 255\n",
    "        else:\n",
    "            # Create a np.array with the correct dimensions\n",
    "            # fill the pixels with random values\n",
    "            trigger_array = (np.random.random((self.dims)))\n",
    "\n",
    "        return trigger_array\n",
    "\n",
    "    def apply_trigger(self, img):\n",
    "        \"\"\"applies the trigger on the image.\"\"\"\n",
    "        # TODO: Apply the crafted trigger on the given image. Keep in mind that the\n",
    "        # pixel values should be inside the permitted limits.\n",
    "        img = ....\n",
    "        return img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc5434-f38b-49b9-b253-e03b2944cca9",
   "metadata": {},
   "source": [
    "### Source-agnostic vs source-specific\n",
    "\n",
    "In **source-agnostic** attacks the trigger is effective in any image, regardless its original class.\n",
    "In **source-specific** attacks the trigger is effective only for a specific original class. If the trigger is applied in any other input the model should classify the poisoned input correctly.\n",
    "\n",
    "Now you should use the code above to poison the training data. Again, decide on the following settings:\n",
    "- source specific: Do you want the attack to be source specific, if so then provide a variable with the source label.\n",
    "- backdoor / target label: If an image includes a trigger, what should the target label be?\n",
    "- epsilon: specify what percentage of the training set you want to poison.\n",
    "\n",
    "Do the following when poisoning the training dataset in case of a **source agnostic** attack:\n",
    "- if a training set example is part of the subset you want to poison, then poison the image and provide the backdoor label as its new label.\n",
    "- if a training set example is not part of the subset you want to poison, then use the original image and label.\n",
    "\n",
    "Do the following when poisoning the training dataset in case of a **source specific** attack:\n",
    "\n",
    "- if a trainig set example is part of the subset you want to poison and its original label is equal to the source label, poison the image and provide the backdoor label as its new label.\n",
    "- if a training set example is part of the subset you want to poison and its original label is not equal to the source label, poison the image and provide the original label.\n",
    "- if a training set example is not part of the subset you want to poison, then use the original image and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a hello-kitty trigger object for MNIST \n",
    "trigger_obj = ...\n",
    "\n",
    "target_label = 1\n",
    "source_label = 5\n",
    "training_epsilon = 0.06\n",
    "test_epsilon = None\n",
    "\n",
    "bkdr_train_set = BackdoorDataset(train_set, trigger_obj, epsilon=training_epsilon, target_label=target_label, source_label=source_label)\n",
    "bkdr_train_loader = torch.utils.data.DataLoader(bkdr_train_set, batch_size=batch_size, shuffle=True,\n",
    "                                                num_workers=num_workers)\n",
    "\n",
    "bkdr_val_set = BackdoorDataset(val_set, trigger_obj, epsilon=training_epsilon, target_label=target_label, source_label=source_label)\n",
    "bkdr_val_loader = torch.utils.data.DataLoader(bkdr_val_set, batch_size=batch_size, shuffle=True,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "bkdr_test_set = BackdoorDataset(test_set, trigger_obj, epsilon=test_epsilon, target_label=target_label, train=False)\n",
    "bkdr_test_loader = torch.utils.data.DataLoader(bkdr_test_set,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822f0a0-0cdd-4bf7-bbb5-8d45ce646481",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkdr_model = Model().to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(bkdr_model.parameters(),lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train(bkdr_model, 'bkdr_model_blended', criterion, optimizer, bkdr_train_loader, bkdr_val_loader, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a46d7-30ed-404b-be1f-fae0801475c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(bkdr_train_loader)\n",
    "images, labels = next(dataiter)\n",
    "plot_images(images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4fb91-0c7d-4442-8c0f-c13c4dc2deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(bkdr_train_loader)\n",
    "images, labels = next(dataiter)\n",
    "plot_images(images,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b86ac-560a-4b37-8f7f-9d81765e3636",
   "metadata": {},
   "source": [
    "#### Calculate ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_acc = calculate_ASR(bkdr_model,bkdr_test_loader,target_label=target_label,source_label=source_label,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39abe66f-670b-453a-8fb0-44f62f8c3471",
   "metadata": {},
   "source": [
    "#### Clean Accuracy Drop\n",
    "\n",
    "In order to compute the clean accuracy drop you will need to first train a model on the clean training set and use the ```test()``` method provided earlier on in this notebook to compute the clean accuracy. Then train a model using the backdoored/poisoned training set. Again use the same ```test()```method to compute the accuracy of this backdoored model on the clean test set. Compare the accuracies to compute the accuracy drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ae17a-78c9-434c-ba2d-9172c8a37ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the clean model's accuracy on clean data\n",
    "clean_acc, examples = test(...)\n",
    "# TODO: Compute the accuracy of the poisoned model on clean data\n",
    "bkdr_acc, examples = test(...)\n",
    "\n",
    "# TODO: Calculate the clean accuracy drop using 4 decimals\n",
    "print(f\"Clean Accuracy drop of: {round(..., 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59475f1-8da8-4fed-b8dd-c8fc7be4d3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
